import time
import pandas as pd
import urllib.request
from selenium import webdriver

# Using Selenium to load & scrape html data from tables generated by javascript on Wikipedia's Analytics website


""" TOP VIEWED PAGES ON WIKIPEDIA 2015-2019 """

Years = [2015, 2016, 2017, 2018, 2019]
Months = ['01', '02', '03', '04', '05', '06', '07', '08', '09', '10', '11', '12']
Days = ['01', '02', '03', '04', '05', '06', '07', '08', '09', '10', '11', '12',
        '13','14','15','16','17','18','19','20','21','22','23','24','25','26',
        '27','28','29','30','31']

base_url = "https://tools.wmflabs.org/topviews/?project=en.wikipedia.org&platform=all-access&date="
extension = "{year}-{date}&excludes="


""" SCRAPE PAGE DATA USING FIREFOX WEBDRIVER """
# Create the master dataframe
all_data = pd.DataFrame()
month_in_2015 = 0

# Record any dates that fail to be scraped
missing_dates = []

# Executes a search for each month and year in the Topviews table
for year in Years:
    for month in Months:
        
        # Data only exists beyond July 2015, this skips all earlier months in 2015
        if month_in_2015 < 6:
            month_in_2015+=1
            continue
        
        for day in Days:
            # Run the firefox webdriver from geckodriver executable path
            driver = webdriver.Firefox(executable_path = '/usr/local/bin/geckodriver')
            driver.get(base_url + f"{year}-{month}-{day}&excludes=")
            
            # Sleep for 7s to let the webpage to load/execute js displaying table data for that month/year
            time.sleep(7)
            
            # Execute js cmd to scroll to the bottom of the webpage to make sure data is loaded
            driver.execute_script("window.scrollTo(0, document.body.scrollHeight);var lenOfPage=document.body.scrollHeight;return lenOfPage;")
            
            # Find Page Names & View Counts with xpath
            Names = driver.find_elements_by_xpath("//*[@class='chart-container col-lg-12']//*[@class='table output-table']//*[@class='topview-entries']//*[@class='topview-entry']//*[@class='topview-entry--label-wrapper']//*[@class='topview-entry--label']")
            print('Firefox Webdriver - Number of results', len(Names))
            
            Views = driver.find_elements_by_xpath("//*[@class='chart-container col-lg-12']//*[@class='table output-table']//*[@class='topview-entries']//*[@class='topview-entry']//*[@class='topview-entry--views']")
            print('Firefox Webdriver - Number of results', len(Views))
            
            # if the page fails to load, record the date & skip it
            if len(Names) == 0 or len(Views) == 0:
                missing_dates.append((day,month,year))
                driver.quit()
                continue
            
            # Store the data
            data = []
            page_name = []
            page_views = []
            # Loop over Top 10 most viewed pages for that month
            for i in range(10):
                if i == 10:
                    break
                page_name.append(Names[i].text) 
                page_views.append(int(Views[i].text.replace(',',''))) 
                # append dict to array
            data.append({"Year":year,"Month":month,"Day":day,"Page Name":page_name,"Monthly Views":page_views})
            
            # Save the data to a dataframe and append it to the master dataframe
            df = pd.DataFrame(data)
            all_data = all_data.append(df)
            
            # Close the driver
            driver.quit()
   
    
""" EXPORT THE DATA """
all_data.index = np.linspace(0,len(all_data.index)-1,len(all_data.index)).astype(int) # Correct the dataframe indexing
all_data.to_csv("topviews.csv")
