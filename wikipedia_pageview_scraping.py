import time
import numpy as np
import pandas as pd
import urllib.request
from selenium import webdriver

# Using Selenium to load & scrape data from dynamic tables generated by JavaScript on Wikipedia's Analytics website

""" DEFINE SCRAPING FUNCTION """

def scrape_data(day,month,year):
    # Scrape the data using the Firefox webdriver via selenium
    driver = webdriver.Firefox(executable_path = '/usr/local/bin/geckodriver')
    driver.get(base_url + f"{year}-{month}-{day}&excludes=")
            
    # Sleep for 7s to let the webpage to load/execute js displaying table data for that month/year
    time.sleep(7)
    # Execute js cmd to scroll to the bottom of the webpage to make sure data is loaded
    driver.execute_script("window.scrollTo(0, document.body.scrollHeight);var lenOfPage=document.body.scrollHeight;return lenOfPage;")
    
    # Find Page Names & View Counts with xpath
    Names = driver.find_elements_by_xpath("//*[@class='chart-container col-lg-12']//*[@class='table output-table']//*[@class='topview-entries']//*[@class='topview-entry']//*[@class='topview-entry--label-wrapper']//*[@class='topview-entry--label']")
    print('Firefox Webdriver - Number of results', len(Names))
    Views = driver.find_elements_by_xpath("//*[@class='chart-container col-lg-12']//*[@class='table output-table']//*[@class='topview-entries']//*[@class='topview-entry']//*[@class='topview-entry--views']")
    print('Firefox Webdriver - Number of results', len(Views))
    
    # If the page fails to load, recursively load it until data is successfully scraped
    if len(Names) == 0 or len(Views) == 0:
        return(scrape_data(day,month,year) )
    
    # Store the data
    df = pd.DataFrame({"Year":year,"Month":month,"Day":day}, index=[0])
    
    # Loop over Top 10 most viewed pages for that month using two loops for desired data structure
    num_pages = 10
    for i in range(num_pages):
        df[f"Name{i}"] = Names[i].text
    for i in range(num_pages):    
        df[f"Views{i}"] = Views[i].text.replace(',','')

    # Close the driver
    driver.quit()
    return(df)


""" CREATE LISTS OF DATES TO BUILD URLS FROM """

Days = [round(np.linspace(0.1,3.1,31)[i],1) for i in range(31)] # list of dates as floats
Days = [str(Days[i]).replace('.','') for i in range(31)] # convert to str & remove decimals
Months = Days[:12]
Years = [2015, 2016, 2017, 2018, 2019]

base_url = "https://tools.wmflabs.org/topviews/?project=en.wikipedia.org&platform=all-access&date="
extension = "{year}-{month}-{day}&excludes="


""" LOOP TRHOUGH AND SCRAPE DATA FOR EACH DATE """

# Create the master dataframe
all_data = pd.DataFrame()

# Executes a search for each day, month, and year in the Wikipedia Topviews table
for year in Years:
    for month in Months:
        # Data only exists beyond July 2015, this skips all prior months in 2015
        if year == 2015 and int(month) < 7:
            continue
        for day in Days:
            all_data = all_data.append(scrape_data(day,month,year))
            
    
""" EXPORT THE DATA """

all_data.index = np.linspace(0,len(all_data.index)-1,len(all_data.index)).astype(int) # Correct the dataframe indexing
all_data.to_csv("pageviews.csv")
